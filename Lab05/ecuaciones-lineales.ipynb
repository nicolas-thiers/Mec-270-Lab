{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "![image](./Images/logo-mecanica.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6fe3a0",
   "metadata": {},
   "source": [
    "# Laboratorio 5: Resolución de ecuaciones lineales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "Este notebook repasa los **métodos numéricos directos e iterativos para encontrar la solución de sistemas de ecuaciones lineales**. Se incluye una breve revisión de la teoría y los algoritmos generales de cada método, seguida de ejercicios prácticos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-review",
   "metadata": {},
   "source": [
    "## 1. Repaso de la teoría"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-problem",
   "metadata": {},
   "source": [
    "**Problema General:** Encontrar el vector $\\vec{x}$ que satisface el **sistema de ecuaciones lineales**: $A\\vec{x} = \\vec{b}$\n",
    "\n",
    "Aquí, $A$ es una matriz cuadrada de tamaño $n \\times n$ y $\\vec{b}$ es un vector conocido. La solución buscada es $\\vec{x}$. Resolver el sistema implica encontrar el vector $\\vec{x} = A^{-1}\\vec{b}$. Sin embargo, calcular la inversa de la matriz $A$ tiene un costo computacional elevado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "direct-methods",
   "metadata": {},
   "source": [
    "### 1.1 Métodos Directos\n",
    "\n",
    "La idea principal de los métodos directos es **transformar el sistema de ecuaciones lineales original $A\\vec{x} = \\vec{b}$ en un sistema equivalente que sea más fácil de resolver**. Esto se logra pre-multiplicando (operando por la izquierda) por una matriz $P$ no singular: $P \\cdot A\\vec{x} = P \\cdot \\vec{b}$ . Dado que $P$ es no singular, $P^{-1}$ existe, y si $\\vec{z}$ es la solución del sistema transformado $M A \\vec{z} = M \\vec{b}$, entonces $\\vec{z} = A^{-1} \\vec{b} = \\vec{x}$, demostrando que la solución no cambia.\n",
    "\n",
    "**Transformaciones útiles** (pre-multiplicación, no modifican la solución):\n",
    "*   **Permutación:** Reordenamiento de las filas del sistema de ecuaciones lineales. Se realiza mediante una matriz de permutación $P$.\n",
    "*   **Escalamiento diagonal:** Multiplicación de cada fila por un escalar distinto de cero. Se realiza mediante una matriz diagonal $D$.\n",
    "\n",
    "**Sistemas Equivalentes Fáciles de Resolver**:\n",
    "*   **Diagonal:** La matriz asociada es diagonal.\n",
    "*   **Tri-diagonal:** La matriz asociada solo tiene elementos en la diagonal principal y en las diagonales inmediatamente superior e inferior.\n",
    "*   **Triangular superior:** La matriz asociada $U$ tiene ceros debajo de la diagonal principal. Un sistema $U\\vec{x} = \\vec{b}$ se resuelve eficientemente mediante **sustitución hacia atrás**. El algoritmo es:\n",
    "\n",
    "```\n",
    "  for j = n to 1:\n",
    "    if u_jj == 0 -> stop\n",
    "    x_j = b_j / u_jj\n",
    "    for i = 1 to j-1:\n",
    "      b_i = b_i - u_ij * x_j\n",
    "    end\n",
    "  end\n",
    "```\n",
    "*   **Triangular inferior:** La matriz asociada $L$ tiene ceros encima de la diagonal principal. Un sistema $L\\vec{x} = \\vec{b}$ se resuelve eficientemente mediante **sustitución hacia adelante**. El algoritmo es:\n",
    "\n",
    "```\n",
    "  for j = 1 to n:\n",
    "    if l_jj == 0 -> stop\n",
    "    x_j = b_j / l_jj\n",
    "    for i = j+1 to n:\n",
    "      b_i = b_i - l_ij * x_j\n",
    "    end\n",
    "  end\n",
    "```\n",
    "\n",
    "#### Eliminación Gaussiana y Factorización LU\n",
    "\n",
    "El objetivo es transformar la matriz $A$ en una matriz triangular superior mediante operaciones que reemplacen elementos por ceros. Estas operaciones se basan en la combinación lineal de filas y pueden representarse mediante **matrices de eliminación elemental** $M_k$. Una matriz $M_k$ puede transformar un vector columna $\\vec{a}$ en un vector con ceros a partir de la fila $k+1$.\n",
    "\n",
    "La **Eliminación Gaussiana** es un método para resolver $A\\vec{x} = \\vec{b}$ transformando el sistema original en un sistema triangular utilizando matrices de eliminación elemental. Aplica secuencialmente matrices de eliminación elemental a la matriz $A$ utilizando los vectores columna como pivotes. La secuencia de operaciones es:\n",
    "\n",
    "$$ A\\vec{x} = \\vec{b} \\Rightarrow M_1 A\\vec{x} = M_1 \\vec{b} \\Rightarrow M_2 M_1 A\\vec{x} = M_2 M_1 \\vec{b} \\Rightarrow \\dots \\Rightarrow M_n M_{n-1} \\dots M_1 A\\vec{x} = M_n M_{n-1} \\dots M_1 \\vec{b} $$\n",
    "\n",
    "Si definimos $M = M_n M_{n-1} \\dots M_1$, entonces $M A = U$, donde $U$ es una matriz triangular superior, y el sistema transformado es $U\\vec{x} = M\\vec{b}$. Este sistema triangular se resuelve luego mediante sustitución hacia atrás.\n",
    "\n",
    "La inversa de la matriz de transformación $M$ es $M^{-1} = L$, donde $L$ es una matriz triangular inferior con unos en la diagonal. De la relación $MA=U$, pre-multiplicando por $M^{-1}$ obtenemos $A = M^{-1} U = L U$. Esta es la **factorización LU** de la matriz $A$. Resolver $A\\vec{x} = \\vec{b}$ se convierte en resolver $L U \\vec{x} = \\vec{b}$. Esto se hace en dos pasos: 1) Resolver $L\\vec{y} = \\vec{b}$ para $\\vec{y}$ (sustitución hacia adelante). 2) Resolver $U\\vec{x} = \\vec{y}$ para $\\vec{x}$ (sustitución hacia atrás).\n",
    "\n",
    "El algoritmo básico para la factorización LU mediante eliminación Gaussiana es:\n",
    "\n",
    "```\n",
    "  for k = 1 to n-1:\n",
    "    if a_kk == 0 -> stop\n",
    "    for i = k+1 to n:\n",
    "      m_ik = a_ik / a_kk\n",
    "    end\n",
    "    for j = k+1 to n:\n",
    "      for i = k+1 to n:\n",
    "        a_ij = a_ij - m_ik * a_kj\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a1971-c215-4d06-98c9-5b74f85461de",
   "metadata": {},
   "source": [
    "### 1.2 Métodos Iterativos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iterative-methods",
   "metadata": {},
   "source": [
    "Resolver el sistema $A\\vec{x} = \\vec{b}$ calculando $A^{-1}$ es de elevado costo. Los métodos directos como la factorización LU son una alternativa. Sin embargo, para **sistemas grandes**, los costos de memoria y computacionales de los métodos directos pueden ser prohibitivos. En estos casos, o para **sistemas matriciales especiales**, es posible utilizar un **método iterativo que converge asintóticamente a la solución**.\n",
    "\n",
    "Los métodos iterativos presentan una ventaja computacional para:\n",
    "*   Matrices diagonales dominantes (DD) ($|a_{ii}| > \\sum_{j \\neq i} |a_{ij}|$).\n",
    "*   Matrices simétricas y definidas positivas (SDP) ($A^T = A$ y $\\vec{x}^T A \\vec{x} > 0$ para todo $\\vec{x} \\neq \\vec{0}$).\n",
    "*   Matrices sparse (con un número reducido de elementos no nulos).\n",
    "*   Matrices triangulares (superior o inferior).\n",
    "*   Matrices bandwidth ($a_{ij}=0$ si $|i-j| > k$).\n",
    "*   Cuando es fácil estimar una solución inicial.\n",
    "\n",
    "La implementación de métodos numéricos para resolver EDPs transientes a menudo produce sistemas matriciales sparse y simétricos, donde la solución del tiempo pasado puede usarse como una buena aproximación inicial para el tiempo presente.\n",
    "\n",
    "#### Iteración Estacionaria\n",
    "\n",
    "El esquema iterativo más sencillo es el de iteración estacionaria:\n",
    "\n",
    "$$ \\vec{x}^{(k+1)} = T\\vec{x}^{(k)} + \\vec{c} $$\n",
    "\n",
    "Donde la matriz $T$ y el vector $\\vec{c}$ se eligen de forma que un punto fijo de la iteración ($\\vec{x}^* = T\\vec{x}^* + \\vec{c}$) sea la solución del sistema $A\\vec{x} = \\vec{b}$ . Se denomina estacionario porque $T$ y $\\vec{c}$ no dependen de la iteración $k$.\n",
    "\n",
    "La matriz $T$ se construye separando la matriz $A$ en $A = M - N$, donde $M$ es una matriz no singular y fácil de invertir. Entonces, $T = M^{-1} N$ y $\\vec{c} = M^{-1} \\vec{b}$. La iteración se puede escribir como $M \\vec{x}^{(k+1)} = N \\vec{x}^{(k)} + \\vec{b}$, lo que implica resolver un sistema lineal con matriz $M$ en cada iteración. La idea es que $M$ sea similar a $A$ pero fácil de invertir (e.g., diagonal, triangular inferior o superior).\n",
    "\n",
    "La convergencia del esquema iterativo está garantizada si el radio espectral de la matriz Jacobiana asociada $J = \\partial G / \\partial \\vec{x} = T = M^{-1}N$ cumple $\\rho(J) < 1$. A menor radio espectral, mayor convergencia. Hay un balance entre la facilidad de invertir $M$ y el radio espectral. El caso extremo $M=A$ converge en una iteración, pero es computacionalmente inviable si $A$ es grande.\n",
    "\n",
    "#### Método de Jacobi\n",
    "\n",
    "Es un método de iteración estacionaria. Se escoge la matriz $M$ como la diagonal de $A$, $M = D = \\text{diag}(A)$, y $-N$ como el resto de la matriz, $N = -(L + U)$, donde $L$ y $U$ son las partes triangular inferior y superior de $A$ respectivamente. Si los elementos diagonales $a_{ii}$ son no nulos, $D$ es invertible.\n",
    "\n",
    "El esquema iterativo es:\n",
    "\n",
    "$$ \\vec{x}^{(k+1)} = D^{-1} (L + U) \\vec{x}^{(k)} + D^{-1} \\vec{b} $$\n",
    "\n",
    "La convergencia está garantizada si la matriz $A$ es diagonal dominante o simétrica definida positiva.\n",
    "\n",
    "El algoritmo del método de Jacobi es:\n",
    "\n",
    "```\n",
    "  x(0) = ... (aproximación inicial)\n",
    "  res = ||A x(0) - b||_p\n",
    "  k = 0\n",
    "  while (res > tol or k < kmax) do\n",
    "    for i = 1, 2, ..., n do\n",
    "      x(k+1)_i = (b_i - sum_{j=1; j!=i}^{n} a_ij * x(k)_j) / a_ii\n",
    "    end for\n",
    "    res = ||A x(k+1) - b||_p\n",
    "    k += 1\n",
    "  end for\n",
    "```\n",
    "\n",
    "#### Método de Gauss-Seidel\n",
    "\n",
    "Una limitación del método de Jacobi es su lenta convergencia, en parte porque no utiliza la información de la iteración actual tan pronto como está disponible. El método de Gauss-Seidel es una modificación que utiliza la información de la iteración actual para calcular la nueva aproximación.\n",
    "\n",
    "Se escoge la matriz $M$ como la suma de la diagonal y la parte triangular inferior de $A$, $M = L + D$, y $-N$ como la parte triangular superior, $N = -U$.\n",
    "\n",
    "El esquema iterativo es:\n",
    "\n",
    "$$ \\vec{x}^{(k+1)} = (L + D)^{-1} (-U \\vec{x}^{(k)} + \\vec{b}) $$\n",
    "\n",
    "Partiendo de $\\vec{x}^{(0)}$, se aproxima la solución resolviendo un sistema lineal con matriz triangular inferior $(L+D)$. En forma de componentes, la fórmula es:\n",
    "\n",
    "$$ x_i^{(k+1)} = \\frac{b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^{n} a_{ij} x_j^{(k)}}{a_{ii}} $$\n",
    "\n",
    "Observe que para calcular $x_i^{(k+1)}$ se utilizan los valores de $x_j^{(k+1)}$ que ya han sido actualizados en la iteración actual para $j < i$, y los valores de $x_j^{(k)}$ de la iteración anterior para $j > i$.\n",
    "\n",
    "La convergencia está garantizada si la matriz $A$ es simétrica definida positiva.\n",
    "\n",
    "El algoritmo del método de Gauss-Seidel es:\n",
    "\n",
    "```\n",
    "  x = ... (aproximación inicial)\n",
    "  res = ||A x - b||_p\n",
    "  k = 0\n",
    "  while (res > tol or k < kmax) do\n",
    "    for i = 1, 2, ..., n do\n",
    "      x_i = (b_i - sum_{j=1}^{i-1} a_ij * x_j - sum_{j=i+1}^{n} a_ij * x_j) / a_ii\n",
    "    end for\n",
    "    res = ||A x - b||_p\n",
    "    k += 1\n",
    "  end for\n",
    "```\n",
    "\n",
    "#### Método del Gradiente Conjugado (CG - Conjugate Gradient)\n",
    "\n",
    "El método del gradiente conjugado es un método iterativo (aunque por construcción es directo) basado en procesos de optimización. Requiere que la matriz $A$ sea **simétrica definida positiva**.\n",
    "\n",
    "La solución del sistema $A\\vec{x} = \\vec{b}$ minimiza la función cuadrática $\\psi(\\vec{x}) = \\frac{1}{2} \\vec{x}^T A \\vec{x} - \\vec{x}^T \\vec{b}$, ya que su gradiente $\\nabla\\psi(\\vec{x}) = A\\vec{x} - \\vec{b}$ es cero en la solución.\n",
    "\n",
    "La idea es moverse desde un punto inicial $\\vec{x}^{(0)}$ hacia el mínimo mediante pasos discretos en una dirección $\\vec{s}^{(k)}$ una distancia $\\alpha$:\n",
    "\n",
    "$$ \\vec{x}^{(k+1)} = \\vec{x}^{(k)} + \\alpha \\vec{s}^{(k)} $$\n",
    "\n",
    "La distancia $\\alpha$ óptima para cualquier dirección de búsqueda $\\vec{s}^{(k)}$ es aquella que hace que el nuevo vector residuo $\\vec{r}^{(k+1)} = \\vec{b} - A\\vec{x}^{(k+1)}$ sea perpendicular a la dirección de búsqueda $\\vec{s}^{(k)}$: $\\vec{r}^{(k+1)T} \\vec{s}^{(k)} = 0$. El residuo se actualiza como $\\vec{r}^{(k+1)} = \\vec{r}^{(k)} - \\alpha A\\vec{s}^{(k)}$. El paso óptimo $\\alpha$ se calcula como:\n",
    "\n",
    "$$ \\alpha = \\frac{\\vec{r}^{(k)T} \\vec{s}^{(k)}}{\\vec{s}^{(k)T} A \\vec{s}^{(k)}} $$\n",
    "\n",
    "Una elección simple para la dirección de búsqueda sería el negativo del gradiente, $\\vec{s}^{(k)} = -\\nabla\\psi(\\vec{x}^{(k)}) = \\vec{r}^{(k)}$ (método del descenso del gradiente). Sin embargo, esto puede llevar a convergencia sub-óptima. El método del gradiente conjugado utiliza direcciones de búsqueda que son **ortogonales respecto a la matriz A (A-conjugadas)**: $\\vec{s}^{(k+1)T} A \\vec{s}^{(k)} = 0$.\n",
    "\n",
    "El algoritmo del método de gradiente conjugado es:\n",
    "\n",
    "```\n",
    "  x(0) = ... (aproximación inicial)\n",
    "  r(0) = b - A * x(0)\n",
    "  s(0) = r(0)\n",
    "  k = 0\n",
    "  while (||r(k)||_p > tol or k < kmax) do\n",
    "    alpha = (r(k)' * r(k)) / (s(k)' * A * s(k))\n",
    "    x(k+1) = x(k) + alpha * s(k)\n",
    "    r(k+1) = r(k) - alpha * A * s(k)\n",
    "    beta = (r(k+1)' * r(k+1)) / (r(k)' * r(k))\n",
    "    s(k+1) = r(k+1) + beta * s(k)\n",
    "    k += 1\n",
    "  end while\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placeholder-lu",
   "metadata": {},
   "outputs": [],
   "source": [
    "### def SolveEliminacionGauss(A_, b_):\n",
    "###     # Implementación de eliminacion Gaussiana\n",
    "###     return xHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placeholder-jacobi",
   "metadata": {},
   "outputs": [],
   "source": [
    "### def SolveJacobi(A_, b_, x0_, tol_=1e-6, kmax_=1000):\n",
    "###     # Implementación del Método de Jacobi\n",
    "###     return xHat, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placeholder-gauss-seidel",
   "metadata": {},
   "outputs": [],
   "source": [
    "### def SolveGaussSeidel(A_, b_, x0_, tol_=1e-6, kmax_=1000):\n",
    "###     # Implementación del Método de Gauss-Seidel\n",
    "###     return xHat, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placeholder-cg",
   "metadata": {},
   "outputs": [],
   "source": [
    "### def SolveCG(A_, b_, x0_, tol_=1e-6, kmax_=None):\n",
    "###     # Implementación del Método de Gradiente Conjugado\n",
    "###     # Requiere que A sea Simétrica Definida Positiva\n",
    "###     return xHat, k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## 2. Ejercicios propuestos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercise-description",
   "metadata": {},
   "source": [
    "### Distribución de Temperaturas en una Barra (Conducción 1D en Estado Estacionario)\n",
    "\n",
    "Considere una barra metálica de longitud $L$ con área de sección transversal constante $A_c$, conductividad térmica $k$. La barra está aislada en sus superficies laterales y sufre convección con el ambiente a temperatura $T_\\infty$ con un coeficiente de convección $h$. Los extremos de la barra se mantienen a temperaturas fijas $T_0$ en $x=0$ y $T_L$ en $x=L$.\n",
    "\n",
    "![image](./Images/Esquema-1.svg)\n",
    "\n",
    "La ecuación de gobierno para la distribución de temperatura $T(x)$ en estado estacionario es:\n",
    "\n",
    "$$ \\frac{d}{dx} \\left( k \\frac{dT}{dx} \\right) - \\frac{h P}{A_c} (T - T_\\infty) = 0 $$\n",
    "\n",
    "donde $P$ es el perímetro de la sección transversal.\n",
    "\n",
    "Utilizaremos el método de diferencias finitas para discretizar esta ecuación en $N$ nodos espaciados uniformemente a lo largo de la barra ($x_i = i \\Delta x$, donde $\\Delta x = L/(N-1)$). Aproximando las derivadas segundas:\n",
    "\n",
    "$$ \\frac{d^2T}{dx^2} \\approx \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} $$\n",
    "\n",
    "Asumiendo conductividad $k$ constante y definiendo $m^2 = h P / (k A_c)$, la ecuación discretizada en el nodo $i$ (para $i=1, \\dots, N-2$) es:\n",
    "\n",
    "$$ k \\frac{T_{i-1} - 2T_i + T_{i+1}}{(\\Delta x)^2} - m^2 k (T_i - T_\\infty) = 0 $$\n",
    "\n",
    "Reordenando, obtenemos una ecuación lineal para cada nodo interior $i$:\n",
    "\n",
    "$$ T_{i-1} - (2 + m^2 (\\Delta x)^2) T_i + T_{i+1} = -m^2 (\\Delta x)^2 T_\\infty $$\n",
    "\n",
    "Para los nodos en los extremos ($i=0$ e $i=N-1$), las condiciones de frontera son simplemente $T_0$ y $T_{N-1} = T_L$.\n",
    "\n",
    "Esto nos lleva a un sistema de $N$ ecuaciones lineales con $N$ incógnitas ($T_0, T_1, \\dots, T_{N-1}$) de la forma $A\\vec{T} = \\vec{b}$. Para $N=5$ nodos, el sistema se ve así:\n",
    "\n",
    "$$i=0: T_0 = T_{\\text{frontera},0}$$\n",
    "\n",
    "$$i=1: T_0 - (2 + m^2 (\\Delta x)^2) T_1 + T_2 = -m^2 (\\Delta x)^2 T_\\infty$$\n",
    "\n",
    "$$i=2: T_1 - (2 + m^2 (\\Delta x)^2) T_2 + T_3 = -m^2 (\\Delta x)^2 T_\\infty$$\n",
    "\n",
    "$$i=3: T_2 - (2 + m^2 (\\Delta x)^2) T_3 + T_4 = -m^2 (\\Delta x)^2 T_\\infty$$\n",
    "\n",
    "$$i=4: T_4 = T_{\\text{frontera},L}$$\n",
    "\n",
    "Sustituyendo las condiciones de frontera en las ecuaciones para $i=1$ y $i=N-2=3$:\n",
    "\n",
    "$$i=1: - (2 + m^2 (\\Delta x)^2) T_1 + T_2 = -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},0}$$\n",
    "\n",
    "$$i=3: T_2 - (2 + m^2 (\\Delta x)^2) T_3 = -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},L}$$\n",
    "\n",
    "El sistema para los $N-2$ nodos interiores ($T_1, T_2, T_3$) es:\n",
    "\n",
    "$$\\begin{bmatrix} -(2 + m^2 (\\Delta x)^2) & 1 & 0 \\\\ 1 & -(2 + m^2 (\\Delta x)^2) & 1 \\\\ 0 & 1 & -(2 + m^2 (\\Delta x)^2) \\end{bmatrix} \\begin{bmatrix} T_1 \\\\ T_2 \\\\ T_3 \\end{bmatrix} = \\begin{bmatrix} -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},0} \\\\ -m^2 (\\Delta x)^2 T_\\infty \\\\ -m^2 (\\Delta x)^2 T_\\infty - T_{\\text{frontera},L} \\end{bmatrix}$$\n",
    "\n",
    "Defina $C = 2 + m^2 (\\Delta x)^2$ y $R = -m^2 (\\Delta x)^2 T_\\infty$. El sistema es:\n",
    "\n",
    "$$ \\begin{bmatrix} -C & 1 & 0 \\\\ 1 & -C & 1 \\\\ 0 & 1 & -C \\end{bmatrix} \\begin{bmatrix} T_1 \\\\ T_2 \\\\ T_3 \\end{bmatrix} = \\begin{bmatrix} R - T_{\\text{frontera},0} \\\\ R \\\\ R - T_{\\text{frontera},L} \\end{bmatrix} $$\n",
    "\n",
    "Este es un sistema de 3x3 para los nodos interiores. La matriz del sistema es **tri-diagonal**, lo que la hace un tipo de matriz especial. Para un número mayor de nodos $N$, la matriz del sistema para los $N-2$ nodos interiores será de tamaño $(N-2) \\times (N-2)$ y también tri-diagonal.\n",
    "\n",
    "**Datos:**\n",
    "\n",
    "1.  Longitud de la barra $L = 1$ m\n",
    "2.  Parámetro $m^2 = 10$ m$^{-2}$\n",
    "3.  Temperatura ambiente $T_\\infty = 20$ °C\n",
    "4.  Temperatura en $x=0$, $T_0 = 100$ °C\n",
    "5.  Temperatura en $x=L$, $T_L = 50$ °C\n",
    "6.  Número de nodos $N=10$\n",
    "7.  Tolerancia para métodos iterativos $tol = 10^{-6}$\n",
    "8.  Máximo de iteraciones para métodos iterativos $k_{max} = 1000$\n",
    "\n",
    "**Tareas:**\n",
    "\n",
    "1.  Calcule el tamaño de la matriz del sistema lineal para los nodos interiores ($N-2$) y el valor de $\\Delta x$.\n",
    "2.  Construya la matriz $A$ y el vector $\\vec{b}$ para el sistema de $N-2$ ecuaciones lineales para los nodos interiores.\n",
    "3.  Resuelva el sistema utilizando un **método directo**. Reporte las temperaturas $T_1, T_2, \\dots, T_{N-2}$.\n",
    "4.  Resuelva el sistema utilizando los **métodos iterativos** de Jacobi, Gauss-Seidel . Use un vector inicial de ceros (o todos unos). Reporte las temperaturas $T_1, \\dots, T_{N-2}$ y el número de iteraciones necesarias para alcanzar la tolerancia $tol$. Calcule el residuo final $||A\\hat{x} - \\vec{b}||_p$.\n",
    "5.  Resuelva el sistema utilizando el **método del Gradiente Conjugado**. Asegúrese de que la matriz $A$ sea simétrica definida positiva para aplicar este método. Reporte las temperaturas y el número de iteraciones. Compare la convergencia (respecto al número de iteraciones) con Jacobi y Gauss-Seidel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exercise-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alumno..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
